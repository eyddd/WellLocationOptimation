{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc93d6b-46f0-47f7-b52a-747cf5e5a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import important libraries \n",
    "import keras\n",
    "import seaborn as sns\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate, AveragePooling2D,SeparableConv2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Activation,BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from scipy.stats import boxcox\n",
    "from PIL import Image\n",
    "from tensorflow.keras import backend as K\n",
    "import re\n",
    "import os\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = MinMaxScaler()\n",
    "scaler4 = MinMaxScaler()\n",
    "scaler5 = MinMaxScaler()\n",
    "scaler6 = MinMaxScaler()\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import model_from_json\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18f1a1-9918-42b6-b80a-74c81df95e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('.......') #Read Datafrom file \n",
    "\n",
    "df1=df[['PRODX','PRODY','INJX','INJY']] #extract well coordinates only (x-train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94863d64-9ff9-49d5-b694-452ed5902bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['COP']\n",
    "plt.hist(data, bins=20, color='blue', alpha=0.7)\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Histogram of Data')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the data distribution in a histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffbfb4-9f2f-487d-a7e4-f8dd83d07b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images of realization\n",
    "for i in range(len(df1)):\n",
    "    image2 = Image.open(df['R'][i])\n",
    "\n",
    "    image2.save(f\".......\")\n",
    "image_data_directory = '.......'\n",
    "\n",
    "# Get a list of all image files in the directory\n",
    "all_image_files = [os.path.join(image_data_directory, filename) for filename in os.listdir(image_data_directory)]\n",
    "all_image_files.sort(key=lambda x: int(re.search(r'\\d+', os.path.basename(x)).group()))\n",
    "pics = all_image_files\n",
    "\n",
    "\n",
    "train_pics = []\n",
    "#Read images and normalize the values\n",
    "for i in range(len(pics)):\n",
    "    img = cv2.imread(pics[i] )  #,cv2.IMREAD_GRAYSCALE\n",
    "    img=cv2.resize(img, (60,60))/255\n",
    "    train_pics.append(img)\n",
    "    \n",
    "train_picss= np.array(train_pics)\n",
    "train_picss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4336a4-8ba4-494a-b193-0b2f69767607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplit the data into 60% Training, 20% Validation, 20% Testing\n",
    "split = train_test_split(df['COP'], train_picss,df1, test_size=0.4, random_state=42)\n",
    "(y_train,y_test1, pic_train, pic_test1,cord_train,cord_test1) = split\n",
    "split1 = train_test_split(y_test1, pic_test1,cord_test1, test_size=0.5, random_state=42)\n",
    "(y_test,y_val, pic_test, pic_val,cord_test,cord_val) = split1\n",
    "len(pic_train), len(y_train), len(pic_test),len(pic_test), len(y_test), len(cord_train), len(cord_test),len(y_val)\n",
    "\n",
    "split3 = {'Pic Train':len(pic_train), 'Pic Validation': len(pic_val),'Pic Test':len(pic_test),\n",
    "         'cord Train':len(pic_train), 'cord Validation': len(cord_val),'cord Test':len(cord_test),\n",
    "         'y Train':len(y_train), 'y Validation': len(y_val),'y Test':len(y_test)\n",
    "        }\n",
    "\n",
    "\n",
    "my_dict_of_lists = {'data': ['Pic', 'cord', 'COP '],\n",
    "                    'Train': [len(pic_train), len(cord_train), len(y_train)],\n",
    "                    'Val':  [len(pic_val), len(cord_val), len(y_val)],\n",
    "                    'Test':  [len(pic_test), len(pic_test), len(y_test)]\n",
    "                   }\n",
    "\n",
    "# Convert the dictionary of lists to a DataFrame\n",
    "d = pd.DataFrame(my_dict_of_lists)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411682ca-b96a-4612-ac98-b164c7234c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing all well locations \n",
    "cord_train = scaler5.fit_transform(cord_train)\n",
    "cord_test = scaler6.fit_transform(cord_test)\n",
    "cord_val = scaler1.fit_transform(cord_val)\n",
    "y_testt = y_test.values\n",
    "y_testtt = y_testt.reshape(-1,1)\n",
    "testY= scaler2.fit_transform(y_testtt)\n",
    "y_trainn = y_train.values\n",
    "y_trainnn = y_trainn.reshape(-1,1)\n",
    "trainY= scaler3.fit_transform(y_trainnn)\n",
    "y_vall = y_val.values\n",
    "y_valll = y_vall.reshape(-1,1)\n",
    "valY=  scaler4.fit_transform(y_valll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66cde6-f153-49a8-8ad5-18578a9eff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check each set statistics \n",
    "print(\"Training set statistics:\")\n",
    "print(\"Min:\", np.min(trainY))\n",
    "print(\"Max:\", np.max(trainY))\n",
    "print(\"Quartiles:\", np.percentile(trainY, [25, 50, 75]))\n",
    "\n",
    "print(\"\\nValidation set statistics:\")\n",
    "print(\"Min:\", np.min(valY))\n",
    "print(\"Max:\", np.max(valY))\n",
    "print(\"Quartiles:\", np.percentile(valY, [25, 50, 75]))\n",
    "\n",
    "print(\"\\nTest set statistics:\")\n",
    "print(\"Min:\", np.min(testY))\n",
    "print(\"Max:\", np.max(testY))\n",
    "print(\"Quartiles:\", np.percentile(testY, [25, 50, 75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfba74-753f-4732-9ec0-384533f3bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create initial NN model \n",
    "\n",
    "def build_multi_input_model(hp):\n",
    "    # CNN branch\n",
    "    cnn_input = Input(shape=(60, 60, 3))  # Adjust to your image input shape\n",
    "    x = cnn_input\n",
    "    for i in range(1, hp.Int('num_cnn_layers', 1, 4) + 1):  # Loop to add variable number of conv layers\n",
    "        x = Conv2D(filters=hp.Int(f'conv_{i}_filters', 16, 128, step=16),\n",
    "                   kernel_size=hp.Choice(f'conv_{i}_kernel_size', [3, 5]),\n",
    "                   activation=hp.Choice(\"activation\", [\"linear\",\"relu\", \"tanh\"]), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        pool_size = hp.Choice(f'pool_{i}_size', [2, 3,4])\n",
    "        x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "        \n",
    "    cnn_output = Flatten()(x)\n",
    "\n",
    "    # ANN branch\n",
    "    cnn2_input = Input(shape=(60,60,1))  # Adjust to your tabular input shape (number of features)\n",
    "    y = cnn2_input\n",
    "    for i in range(1, hp.Int('num_cnn2_layers', 1, 4) + 1):  # Loop to add variable number of conv layers\n",
    "        y = Conv2D(filters=hp.Int(f'conv2_{i}_filters', 16, 128, step=16),\n",
    "                   kernel_size=hp.Choice(f'conv2_{i}_kernel_size', [3, 5]),\n",
    "                   activation='relu', padding='same')(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        pool_size = hp.Choice(f'pool_{i}_size', [2, 3,4])\n",
    "        y = MaxPooling2D(pool_size=(pool_size, pool_size))(y)\n",
    "        \n",
    "    cnn2_output = Flatten()(y)\n",
    "    for i in range(1, hp.Int('num_ann_layers', 1, 8) + 1):  # Loop to add variable number of dense layers\n",
    "        y = Dense(units=hp.Int(f'cnn2_{i}_units', 16, 512, step=16), activation='relu')(cnn2_output)\n",
    "        if hp.Boolean(f'ann_{i}_dropout'):  # Optionally add a dropout layer\n",
    "            y = Dropout(rate=hp.Float(f'cnn2_{i}_dropout_rate', 0.1, 0.5, step=0.1))(y)\n",
    "    \n",
    "\n",
    "    # Combine branches\n",
    "    combined = concatenate([cnn_output, cnn2_output])\n",
    "\n",
    "    # Fully connected layers after merging\n",
    "    z = combined\n",
    "    for i in range(1, hp.Int('num_fc_layers', 1, 8) + 1):\n",
    "        z = Dense(units=hp.Int(f'fc_{i}_units', 16, 512, step=16), activation=hp.Choice(\"activation\", [\"linear\",\"relu\", \"tanh\"]))(z)\n",
    "        if hp.Boolean(f'fc_{i}_dropout'):  # Optionally add a dropout layer\n",
    "            z = Dropout(rate=hp.Float(f'fc_{i}_dropout_rate', 0.1, 0.5, step=0.1))(z)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation=hp.Choice(\"activation\", [\"linear\",\"relu\", \"tanh\"]))(z)  # Adjust 'num_classes' based on your task\n",
    "\n",
    "    model = Model(inputs=[cnn_input, cnn2_input], outputs=output)\n",
    "\n",
    "    # Compile model\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    # Optionally, you can also tune the learning rate of each optimizer\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile the model with the chosen optimizer, loss, and metrics\n",
    "    model.compile(optimizer=optimizer, loss='msle', metrics=[R2])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "import keras_tuner\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=build_multi_input_model,\n",
    "    # Specify the name and direction of the objective.\n",
    "    objective=keras_tuner.Objective(\"val_R2\", direction=\"max\"),\n",
    "    max_trials=100,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"custom_metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize NN model to get highest R^2\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor the validation loss\n",
    "                               patience=15, # Number of epochs with no improvement after which training will be stopped\n",
    "                               restore_best_weights=True)\n",
    "tuner.search([pic_train,cord_train1], trainY, epochs=200, validation_data=([pic_val,cord_val1], valY), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23891554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the best model from keras tuner \n",
    "tuner_model = tuner.get_best_models(num_models=1)[0]\n",
    "hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "for param in best_hyperparameters.values:\n",
    "    print(f\"{param}: {best_hyperparameters.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea833cf-ae30-466c-abb8-b3ecfdbf2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coeff. Determination \n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa10af-1853-41d1-b74e-810aaebfb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use for retraining if needed\n",
    "pic_all = np.concatenate((pic_train, pic_val))\n",
    "cord_all = np.concatenate((cord_train, cord_val))\n",
    "allY = np.concatenate((trainY, valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867a0b7-6713-485d-a31b-b7eb612b5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN model retraining if needed\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor the validation loss\n",
    "                               patience=15, # Number of epochs with no improvement after which training will be stopped\n",
    "                               restore_best_weights=True)\n",
    "opt = Adam(learning_rate=0.00010904442257839217)\n",
    "tuner_model.compile(loss='msle', optimizer=opt, metrics = [R2,'mae'] )\n",
    "print(\"[INFO] training model...\")\n",
    "history = tuner_model.fit(x=[pic_train,cord_train], y=trainY, \n",
    "     validation_data = ([pic_val,cord_val], valY),\n",
    "        epochs=250, batch_size = 64, callbacks = [early_stopping], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77098b67-0568-4d84-a590-98ce115a3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check overfitting\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1446b61-65b3-4976-9dda-9756f3055bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R^2 testing\n",
    "preds = tuner_model.predict([pic_test,cord_test])\n",
    "data = testY\n",
    "r2 = r2_score(preds, data)\n",
    "\n",
    "print(f'R2 Score : {round(r2,2)}')\n",
    "# Plot true values vs. predicted values\n",
    "\n",
    "plt.scatter(data, preds, label=f'R² = {r2:.2f}')\n",
    "plt.plot([0, 1], [0, 1], '--', color='red', label='Ideal Line')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs. Predicted Values with R² Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2b020-2ed6-4eb0-befd-829142c9364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R^2 training\n",
    "preds = tuner_model.predict([pic_train,cord_train])\n",
    "data = trainY\n",
    "r2 = r2_score(preds, data)\n",
    "\n",
    "print(f'R2 Score : {round(r2,2)}')\n",
    "# Plot true values vs. predicted values\n",
    "\n",
    "plt.scatter(data, preds, label=f'R² = {r2:.2f}')\n",
    "plt.plot([0, 1], [0, 1], '--', color='red', label='Ideal Line')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs. Predicted Values with R² Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1575339-de8c-4be4-bcb1-fcf3c4d895e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess PSO\n",
    "olumns = ['PRODX', 'PRODY', 'INJX','INJY']\n",
    "df2 = pd.DataFrame(scaler5.inverse_transform(np.array(cord_train)), columns = columns)\n",
    "df2\n",
    "maxp = df2['PRODX'].max()\n",
    "minp = df2['PRODX'].min()\n",
    "\n",
    "maxpy = df2['PRODY'].max()\n",
    "minpy = df2['PRODY'].min()\n",
    "\n",
    "maxi = df2['INJX'].max()\n",
    "mini = df2['INJX'].min()\n",
    "\n",
    "maxiy = df2['INJY'].max()\n",
    "miniy = df2['INJY'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b00230-827b-4b65-b59a-a3e86ffcb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate inside reservoir only\n",
    "image = cv2.imread('.......', cv2.IMREAD_GRAYSCALE)\n",
    "image=cv2.resize(image, (60,60))\n",
    "# Threshold the image to get a binary mask\n",
    "# (this threshold value is just an example, you might need to adjust it)\n",
    "_, mask = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Get the dimensions of the image to limit the random point generation\n",
    "height, width = mask.shape\n",
    "\n",
    "# Function to generate a point inside the leaf\n",
    "def generate_and_normalize_well_locations(num_wells):\n",
    "    \"\"\"\n",
    "    Generates multiple sets of well location coordinates that fall within the leaf's area in the mask.\n",
    "    Normalizes these coordinates based on provided min and max values for each axis.\n",
    "    \"\"\"\n",
    "    height, width = mask.shape\n",
    "    all_well_locations = []\n",
    "\n",
    "    for _ in range(num_wells):\n",
    "        well_locations = []\n",
    "        for _ in range(1):  # Generate two wells (four points)\n",
    "            while True:\n",
    "                x1 = np.random.randint(2, width)\n",
    "                y1 = np.random.randint(2, height)\n",
    "                if mask[y1, x1]:  # Assuming leaf area is True in the mask\n",
    "                    # Normalize the coordinates\n",
    "                    px = (x1 - minp) / (maxp - minp)\n",
    "                    py = (y1 - minpy) / (maxpy - minpy)\n",
    "                    well_locations.extend([px, py])\n",
    "                    break\n",
    "        \n",
    "        # Assuming you want to normalize second set of points differently\n",
    "        # using mini, maxi, miniy, maxiy\n",
    "        for _ in range(1):  # Generate two more wells (four more points)\n",
    "            while True:\n",
    "                x = np.random.randint(2, width)\n",
    "                y = np.random.randint(2, height)\n",
    "                if mask[y, x]:  # Check if within leaf\n",
    "                    # Normalize the coordinates\n",
    "                    ix = (x - mini) / (maxi - mini)\n",
    "                    iy = (y - miniy) / (maxiy - miniy)\n",
    "                    well_locations.extend([ix, iy])\n",
    "                    break\n",
    "\n",
    "        all_well_locations.append(well_locations)\n",
    "\n",
    "    return np.array(all_well_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a887b52-4cf3-4968-b948-0766307c7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if inside reservoir only\n",
    "def check_coordinates_in_leaf(coord1, coord2):\n",
    "    x1, y1 = coord1\n",
    "    x2, y2 = coord2\n",
    "    height, width = mask.shape\n",
    "    \n",
    "    # Check if both coordinates are within the image dimensions\n",
    "    x1, y1= int(round(x1)), int(round(y1))\n",
    "    x2, y2= int(round(x2)), int(round(y2))\n",
    "    if 0 <= x1 < width and 0 <= y1 < height and 0 <= x2 < width and 0 <= y2 < height:\n",
    "        # Check if both coordinates fall within the leaf area\n",
    "        if mask[y1, x1] and mask[y2, x2]:\n",
    "            return coord1, coord2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6912f64-abca-4689-bc06-6008303dcbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pics of relizations\n",
    "R76 = train_picss[829:830]\n",
    "R60 = train_picss[1400:1401]\n",
    "R82 = train_picss[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f444b00-8eca-46cb-8028-428292ac8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective function for PSO\n",
    "def objective_function(Xp,Yp,Xi,Yi):\n",
    "    point1=  Xp,Yp\n",
    "    point2 = Xi,Yi \n",
    "    check_coordinates_in_leaf(point1,point2) #Previously defined function to check if wells are inside reservoir\n",
    "    if check_coordinates_in_leaf(point1, point2) == 0:\n",
    "        return 0  # Points are outside the leaf\n",
    "    Xp = (Xp - minp) / (maxp - minp) #Normalizing\n",
    "    Yp = (Yp - minpy) / (maxpy - minpy)\n",
    "    Xi = (Xi - mini) / (maxi - mini)\n",
    "    Yi = (Yi - miniy) / (maxiy - miniy)\n",
    "    well_locations = np.array((Xp,Yp,Xi,Yi))\n",
    "    well_locations = well_locations.reshape(1, 4)\n",
    "    well_locations1 = scaler5.inverse_transform(well_locations.reshape(1,-1))\n",
    "    r82 = tuner_model.predict([R82,well_locations])\n",
    "    r76 = tuner_model.predict([R76,well_locations])\n",
    "    r60 = tuner_model.predict([R60,well_locations])\n",
    "    productivity = (r82 + r76 + r60)/3\n",
    "    return productivity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0c311-3696-4448-8f05-8e17bb8b95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Problem bounds\n",
    "Xp_bounds = (0, 60)\n",
    "Yp_bounds = (0, 60)\n",
    "Xi_bounds = (0, 60)\n",
    "Yi_bounds = (0, 60)\n",
    "\n",
    "# PSO parameters\n",
    "num_particles = 50\n",
    "num_dimensions = 4  # (Xp, Yp), (Xi, Yi)\n",
    "num_iterations = 200\n",
    "w_max = 0.9  # Initial inertia weight\n",
    "w_min = 0.4  # Final inertia weight\n",
    "c1 = 1.79445  # Cognitive (particle) weight\n",
    "c2 = 1.79445  # Social (swarm) weight\n",
    "\n",
    "# Initialize well locations randomly within the bounds of the egg reservoir model\n",
    "particles = np.random.rand(num_particles, num_dimensions)\n",
    "particles *= np.array([Xp_bounds[1], Yp_bounds[1], Xi_bounds[1], Yi_bounds[1]])  # Scale particles to bounds\n",
    "particles = particles.astype(float)  # Ensure particles are floating-point\n",
    "\n",
    "# Initialize velocities as floating-point zeros\n",
    "velocities = np.zeros((num_particles, num_dimensions), dtype=float)\n",
    "\n",
    "# Initialize personal best well coordinates and COP\n",
    "pbest = np.copy(particles)\n",
    "pbest_scores = np.array([objective_function(*p) for p in particles])\n",
    "\n",
    "# Initialize global best well locations(particles) and cop\n",
    "gbest_index = np.argmax(pbest_scores)\n",
    "gbest = particles[gbest_index]\n",
    "gbest_score = pbest_scores[gbest_index]\n",
    "best_obj_values = [] \n",
    "# PSO loop\n",
    "for i in range(num_iterations):\n",
    "    # Dynamic inertia weight adjustment\n",
    "    w = w_max - ((w_max - w_min) * i / num_iterations)\n",
    "    \n",
    "    for j in range(num_particles):\n",
    "        # Update velocities\n",
    "        r1, r2 = np.random.rand(2)  # Random coefficients for stochastic components\n",
    "        velocities[j] = w * velocities[j] + \\\n",
    "                        c1 * r1 * (pbest[j] - particles[j]) + \\\n",
    "                        c2 * r2 * (gbest - particles[j])\n",
    "\n",
    "        # Update particle positions\n",
    "        particles[j] += velocities[j]\n",
    "        velocities[j] =velocities[j]\n",
    "        # Ensure particles remain within bounds (optional, depending on your problem)\n",
    "        # particles[j] = np.clip(particles[j], [Xp_bounds[0], Yp_bounds[0], Xi_bounds[0], Yi_bounds[0]], \n",
    "        #                                       [Xp_bounds[1], Yp_bounds[1], Xi_bounds[1], Yi_bounds[1]])\n",
    "\n",
    "        # Evaluate new positions\n",
    "        score = objective_function(*particles[j])\n",
    "\n",
    "        # Update personal best if the new score is better\n",
    "        if score > pbest_scores[j]:\n",
    "            pbest[j] = particles[j]\n",
    "            pbest_scores[j] = score\n",
    "\n",
    "            # Update global best if the new score is better\n",
    "            if score > gbest_score:\n",
    "                gbest = particles[j]\n",
    "                gbest_score = score\n",
    "    current_best_value = scaler3.inverse_transform(np.array(gbest_score).reshape(-1,1))\n",
    "    \n",
    "    # Store the best objective value of the current iteration\n",
    "    best_obj_values.append(current_best_value)\n",
    "    #Print the best score and position found in this iteration\n",
    "    print(f\"Iteration {i + 1}: Best score = {gbest_score}, Best position = {gbest}\")\n",
    "\n",
    "# Final best solution\n",
    "print(f\"Final best position: {gbest}, Best score: {gbest_score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6b6bb-cd4c-4d2b-9bba-c681a4ec76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler3.inverse_transform(np.array(gbest_score).reshape(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
